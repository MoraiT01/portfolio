# ml-backend

Machine learning backend (ml-backend) contains the docker containers and processing workflows for
processing media files for HAnS.

## Directed Acyclic Graphs (DAGs)

The workflows for HAnS are specified using [Apache Airflow](https://airflow.apache.org/) (airflow)
Directed Acyclic Graphs (DAGs).
For more details on the DAGs concept of airflow, see
[DAGs](https://airflow.apache.org/docs/apache-airflow/stable/concepts/dags.html).
A workflow consists of multiple tasks. For more details about the task concept of airflow, see
[Tasks](https://airflow.apache.org/docs/apache-airflow/stable/concepts/tasks.html).

For HAnS use cases the tasks are mainly consisting of the following operators, hooks or providers:

- BashOperator - executes a bash command
- PythonVirtualEnvOperator - calls an arbitrary Python function
- DockerOperator - executes a docker container on the host docker engine

We use our own connectors to store data on [MinIO](https://min.io) storage assetdb-temp:

- [connector_provider](./dags/modules/connectors/connector_provider.py) -
  provides connector for assetdb-temp based on [minio_connector](./dags/modules/connectors/minio_connector.py)
  - Could be configured using [connections helper](./dags/modules/operators/connections.py)
    in `modules/operators` method `get_assetdb_temp_config`.

We use the [xcom helper](./dags/modules/operators/xcom.py) methods in `modules/operators`
to share data between the operators:

- inject_xcom_data - helper to inject XCom data from another operator of a task group mainly used
  for operators to inject the xcom data they need.
- get_data_from_xcom - helper to parse XCom data structure to return string value
    of given xcom data key.

We organize the workflow grouping operators in [our own task groups](./dags/modules/task_groups/).

All of our custom operators are located in [`dags/modules/operators`](./dags/modules/operators/).

For more details about the operators of airflow, see
[Operators](https://airflow.apache.org/docs/apache-airflow/stable/concepts/operators.html).
For a full list of operators and hooks, see
[Operators and Hooks Reference](https://airflow.apache.org/docs/apache-airflow-providers/operators-and-hooks-ref/index.html).

The subfolder `dags` contains the following workflows:

- hans_v1.py:
  - Default workflow for a HAnS media upload
  - See [Processing Sequence](./../SEQUENCES.md#processing)

- test_dag_v1.py:
  - Test simple task execution on airflow

## Docker Operators

Workflows (defined as DAGs) are using DockerOperators,
e.g. to execute long-running complex machine learning inference tasks.

Common used python scripts in [`dags/docker_jobs_common`](./dags/docker_jobs_common/)
are provided and copied to the corresponding container subfolders below `dags/docker_jobs`
during building of the docker images by the [`start.cmd`](start.cmd) script.

The images are builded before the complete `ml-backend` starts up with docker compose,
so the docker engine only needs to create a container for an existing image during Airflow runtime.

The docker containers used by the DockerOperator are configured in the folder `dags/docker_jobs`:

### asr-engine-local-*

Docker container starting with pattern "asr-engine-local-*" performing
speech to text (stt) processing using a specific local Automatic Speech Recognition (ASR) engine.

- Tasks:
  - Use local ASR engine to perform ASR for input file `raw_audio.wav` (stored in assetdb-temp)
- Supported formats:
  - Audio input format: 16bit 16kHz mono
- Output:
  - JSON file `asr_result.json` stored in assetdb-temp,
    see [Example](./dags/docker_jobs/asr-engine-remote-mod9/asr_result.json)
    - Format:

      ```json
      {
          "type": "AsrResult",
          "result": [
              {
                  "confidence": number,
                  "final": true,
                  "interval": [
                      number,
                      number
                  ],
                  "result_index": integer,
                  "status": string,
                  "transcript": string,
                  "words": []
              }
          ]
      }
      ```

      Members:

      ```json
            "words": [
                {
                    "alternatives": [
                        {
                            "confidence": number,
                            "word": string
                        }
                    ],
                    "confidence": number,
                    "interval": [
                        number,
                        number
                    ],
                    "word": string
                }
            ]
      ```

      The confidence values are generated by the asr engine.

### asr-engine-remote-*

Docker container starting with pattern "asr-engine-remote-*" performing
speech to text (stt) processing using a specific remote Automatic Speech Recognition (ASR) engine,
e.g. [asr-engine-remote-mod9](./dags/docker_jobs/asr-engine-remote-mod9/).

- Tasks:
  - Uses a remote ASR engine, e.g. a cloud service or another server,
    to perform ASR for input file `raw_audio.wav` (stored in assetdb-temp)
- Supported formats:
  - Audio input format: 16bit 16kHz mono
- Output:
  - JSON file `asr_result.json` stored in assetdb-temp,
    see [Example](./dags/docker_jobs/asr-engine-remote-mod9/asr_result.json)
  - JSON format see section [asr-engine-local-*](#asr-engine-local-)

### media-combine-audio-slides (missing)

No video file but an audio file (podcast) is available.

- Task:
  - Combine audio file with the slides images and a timestamp file to a new video file
- Supported formats:
  - Audio input formats: MP3, AAC with 16bit and 16-48kHz mono
  - Presentation slide formats: PDF
  - Timestamp file, contains the page number and the corresponding time interval
    - Format:

      ```json
      {
          "type": "Timestamps",
          "result": [
              {
                  "interval": [
                      number,
                      number
                  ],
                  "result_index": integer,
                  "page": number
              }
          ]
      }
      ```

- Output:
  - Video in H.264/mp4 format stored in assetdb-temp

### media-converter-audio

- Task:
  - Converts the raw video audio track to a 16 bit 16 kHz mono wav file
- Supported formats:
  - Video input format: H.264/mp4
- Output:
  - wav file `raw_audio.wav` stored in assetdb-temp

### media-converter-video

- Task:
  - Convert the video (stored in assetdb-temp) and generate the streaming compatible video files
    - MPD files for [DASH streaming](https://de.wikipedia.org/wiki/Dynamic_Adaptive_Streaming_over_HTTP)
- Supported formats:
  - Video input format: H.264/mp4
- Output:
  - Dash MPD file stored in assetdb-temp
  - mp4s video files stored in assetdb-temp

### media-thumbnail-image

- Task:
  - Create a thumbnail image of the video (stored in assetdb-temp)
- Supported formats:
  - Video input format: H.264/mp4
- Output:
  - thumbnail image file in png format stored in assetdb-temp

### text-extractor (missing)

- Tasks:
  - If slides are available, extract the text for each slide of the PDF file using
    [pdfminer](https://pypi.org/project/pdfminer/)
  - If no slides are available, extract images from the video and detect text using
    [tesseract-ocr](https://tesseract-ocr.github.io/tessdoc/)
- Supported formats:
  - Video input format: H.264/mp4
  - Audio input formats: MP3, AAC with 16bit and 16-48kHz mono
  - Presentation slide formats: PDF
- Output:
  - List of words and their occurences as JSON file `text_result.json` stored in assetdb-temp
    - Format:

      ```json
      {
          "type": "TextResult",
          "result" : [
              {
                  "result_index": integer,
                  "words": [
                      {
                          "word": string,
                          "occurences": integer
                      }
                  ]
              }
          ]
      }
      ```

  - List of words and their occurences as JSON file `ocr_result.json` stored in assetdb-temp
    - Format:

      ```json
      {
          "type": "OcrResult",
          "result" : [
              {
                  "result_index": integer,
                  "interval": [
                      number,
                      number
                  ],
                  "words": [
                      {
                          "word": string
                      }
                  ]
              }
          ]
      }
      ```

### nlp-text-normalization

- Tasks:
  - Normalize recognized words (if not already done by asr-processor) from input JSON file
    [`asr_rsult.json`](#asr-engine-local-) (stored in assetdb-temp)
    - Convert numbers
    - Convert math symbols
    - ...
  - Convert words to sentence structure with corresponding intervals
    - Each `result` item is one sentence with the specific interval and the corresponding words and word intervals
- Supported formats:
  - JSON file [`asr_rsult.json`](#asr-engine-local-)
- Output:
  - Normalized JSON file `asr_result_normalized.json` stored in assetdb-temp
    - Format:

      ```json
      {
          "type": "AsrNormalizedResult",
          "result": [
              {
                  "interval": [
                      number,
                      number
                  ],
                  "result_index": integer,
                  "transcript_formatted": string,
                  "words_formatted": []
              }
          ]
      }
      ```

      Members:

      ```json
            "words_formatted": [
                {
                    "interval": [
                        number,
                        number
                    ],
                    "word": string
                }
            ]
      ```

## Other DockerOperators (planned)

### speaker-detector

- Tasks:
  - Detect speakers from input file `raw_audio.wav` (stored in assetdb-temp)
- Supported formats:
  - Audio input format: 16bit 16kHz mono
- Output:
  - Speaker Ids and corresponding timestamps as JSON file `speakers_result.json` stored in assetdb-temp
    - Format:

      ```json
      {
          "type": "SpeakerIdResult",
          "result" : [
              {
                  "result_index": integer,
                  "interval": [
                      number,
                      number
                  ],
                  "speaker": [
                      {
                          "id": string,
                          "confidence": number
                      }
                  ]
              }
          ]
      }
      ```

### anonymization-processor

- Tasks:
  - Replace all speakers except the main speaker from input file `raw_audio.wav` (stored in assetdb-temp)
    using JSON file `speakers_result.json` (stored in assetdb-temp) with synthesized voices
- Supported formats:
  - Audio input format: 16bit 16kHz mono
  - JSON file `speakers_result.json`
- Output:
  - wav file `raw_audio_anonymized.wav` stored in assetdb-temp

### segmentation-processor

- Tasks:
  - Detects topic segments in input JSON file `asr_result_normalized.json` (stored in assetdb-temp)
- Supported formats:
  - Normalized JSON file `asr_result_normalized.json`
- Output:
  - JSON file `searchindextopics.json` stored in assetdb-temp
    - Format:

      ```json
      {
          "type": "TopicResult",
          "result" : [
              {
                  "result_index": integer,
                  "interval": [
                      number,
                      number
                  ],
                  "topics": [
                      {
                          "id": string,
                          "confidence": number
                      }
                  ]
              }
          ]
      }
      ```

### qa-generation-processor

- Tasks:
  - Generates question and answers for a media file, a collection or chapter and stores them in assetdb
